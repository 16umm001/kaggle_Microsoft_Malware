from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.linear_model import LogisticRegression as LGR
from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.ensemble import ExtraTreesClassifier as ET
from xgboost_c import XGBC
from sklearn import cross_validation
from sklearn.cross_validation import StratifiedKFold as KFold
from sklearn.metrics import log_loss
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfTransformer


# create model_list

def get_model_list():
    model_list = [
        (RF(n_estimators=2000, n_jobs=-1, max_features = "log2",
                     random_state = 1234), 'rf2000log2'),
        (RF(n_estimators=2000, n_jobs=-1, max_features = "sqrt",
                     random_state = 1234), 'rf2000sqrt'),
        (ET(n_estimators=2000, n_jobs=-1, max_features = "log2",
                     random_state = 1234), 'et2000log2'),
        (ET(n_estimators=2000, n_jobs=-1, max_features = "sqrt",
                     random_state = 1234), 'et2000sqrt'),
        (LGR(random_state=1234),'lgr')
    ]

    for num_round in [150, 300]:
        for max_depth in [2, 8, 20, 30]:
            for eta in [0.1,0.25,0.5]:
                for min_child_weight in [2,5]:
                    model_list.append((XGBC(num_round = num_round, max_depth = max_depth, eta = eta, 
                                            min_child_weight = min_child_weight, nthread = 16),
                                       'xgb_tree_%i_depth_%i_lr_%f_child_%i'%(num_round, max_depth, eta, min_child_weight)))
    return model_list


def gen_data():
    # ngram datasets
    train_data = pd.read_csv("train_data_750.csv")
    test_data = pd.read_csv("test_data_750.csv")

    train_label = pd.read_csv("trainLabels.csv")
    
    ### three types: count, count_tf, frequency

    # single byte frequency datasets
    train_count = pd.read_csv("train_frequency.csv")
    test_count = pd.read_csv("test_frequency.csv")

    '''
    train_count_tf = train_count.copy()
    test_count_tf = test_count.copy()

    # tf idf 
    tf_idf = TfidfTransformer()
    train_count_tf.ix[:,1:] = tf_idf.fit_transform(train_count_tf.ix[:,1:]).toarray()
    test_count_tf.ix[:,1:] = tf_idf.transform(test_count_tf.ix[:,1:]).toarray()

    '''
    train_frequency = train_count.copy()
    test_frequency = test_count.copy()
    train_frequency.ix[:,1:] = train_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_frequency = train_frequency.replace(np.inf, 0)
    test_frequency.ix[:,1:]=test_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_frequency = test_frequency.replace(np.inf, 0)


    # daf features
    train_daf = pd.read_csv("train_daf.csv")
    test_daf = pd.read_csv("test_daf.csv")
    daf_list = [0,165,91,60,108,84,42,93,152,100] #daf list for 500 grams.
    #daf_list = [0, 122, 39, 64, 83, 63, 34, 83, 85,86]  # daf list for 1000 grams.


    # three types: count, count_tf, frequency
    # instr freq features
    train_instr_count = pd.read_csv("train_instr_frequency.csv")
    test_instr_count = pd.read_csv("test_instr_frequency.csv")
    for n in list(train_instr_count)[1:]:
        if np.sum(train_instr_count[n]) == 0:
            del train_instr_count[n]
            del test_instr_count[n]
    '''
    train_instr_count_tf = train_instr_count.copy()
    test_instr_count_tf  = test_instr_count.copy()

    tf_idf = TfidfTransformer()
    train_instr_count_tf.ix[:,1:] = tf_idf.fit_transform(train_instr_count_tf.ix[:,1:]).toarray()
    test_instr_count_tf.ix[:,1:] = tf_idf.transform(test_instr_count_tf.ix[:,1:]).toarray()
    '''
    train_instr_freq = train_instr_count.copy()
    test_instr_freq = test_instr_count.copy()

    train_instr_freq.ix[:,1:] = train_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_instr_freq = train_instr_freq.replace(np.inf, 0)
    test_instr_freq.ix[:,1:]=test_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_instr_freq = test_instr_freq.replace(np.inf, 0)


    # dll features
    train_dll = pd.read_csv("train_dll.csv")
    test_dll = pd.read_csv("test_dll.csv")

    # file size 
    train_file_size = pd.read_csv("train_file_size.csv")
    test_file_size = pd.read_csv("test_file_size.csv")


    ## these are results based on single feature set cv results.
    '''
    byte_selection = {1: 'count', 2: 'tf', 3: 'count', 4: 'count',
                    5: 'count',6: 'freq', 7:'count', 8:'freq', 9:'tf'}

    instr_selection = {1: 'count', 2: 'tf', 3: 'tf', 4: 'tf',
                    5: 'count',6: 'freq', 7:'count', 8:'freq', 9:'tf'}
    '''

    class_copy = train_label.copy()
    for c in range(1,10):
        class_copy['Class'] = [int(i == c) for i in train_label['Class']]
        train = pd.merge(train_data.ix[:,[0] + range((c-1)*750+1, c*750+1)], class_copy, on='Id',how='inner')
        test = test_data.ix[:,[0] + range((c-1)*750+1, c*750+1)]
        
        # merge byte frequency
        '''
        if byte_selection[c] == 'count':
            train_byte_features = train_count
            test_byte_features = test_count
        elif byte_selection[c] == 'tf':
            train_byte_features = train_count_tf
            test_byte_features = test_count_tf
        elif byte_selection[c] == 'freq':
            train_byte_features = train_frequency
            test_byte_features = test_frequency
        '''
        train_byte_features = train_frequency
        test_byte_features = test_frequency

        train = pd.merge(train, train_byte_features, on='Id')
        test = pd.merge(test, test_byte_features, on='Id')
        
        # merge daf
        train = pd.merge(train, train_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')
        test = pd.merge(test, test_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')

        '''
        # merge instr freq
        if instr_selection[c] == 'count':
            train_instr_features = train_instr_count
            test_instr_features = test_instr_count
        elif instr_selection[c] == 'tf':
            train_instr_features = train_instr_count_tf
            test_instr_features = test_instr_count_tf
        elif instr_selection[c] == 'freq':
            train_instr_features = train_instr_freq
            test_instr_features = test_instr_freq   
        '''
        train_instr_features = train_instr_freq
        test_instr_features = test_instr_freq   

        train = pd.merge(train, train_instr_features, on='Id')
        test = pd.merge(test, test_instr_features, on='Id')

        # merge dll
        train = pd.merge(train, train_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')
        test = pd.merge(test, test_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')

        # merge file size
        train = pd.merge(train, train_file_size, on='Id')
        test = pd.merge(test, test_file_size, on='Id')        

        yield train, test, c, list(train_daf.ix[:, range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)])


'''
now the three_shots is actually 5 shots! one with best single model, the others are xgb models!
'''
def three_shots(model_list, train, test, c, daf_fea):
    '''
    three_shots includes one cross_validation stacking, one stacking with better half, 
    and one single shot with best cross_validation results.
    '''
    # read in data
    print "read data and prepare modelling..."
    X = train
    Id = X.Id
    labels = X.Class
    del X['Id']
    del X['Class']
    X = X.as_matrix()
    X_test = test
    Id_test = X_test.Id
    del X_test['Id']
    X_test = X_test.as_matrix()
    kf = KFold(labels, n_folds=4)  # 4 folds

    # create cv meta data
    stack_train = np.zeros((len(Id),len(model_list)))
    stack_test = np.zeros((len(Id_test),len(model_list)))

    cv_result = list()  # store the cross validation scores.
    for j, (clf, clf_name) in enumerate(model_list):
        print 'Training classifier %s' %clf_name
        blend_test_tmp = np.zeros((X_test.shape[0],len(kf)))
        for i, (train_fold, validate) in enumerate(kf):
            X_train, X_validate, labels_train, labels_validate = X[train_fold,:], X[validate,:], labels[train_fold], labels[validate]
            clf.fit(X_train,labels_train)
            stack_train[validate,j] = clf.predict_proba(X_validate)[:,1]
            blend_test_tmp[:,i] = clf.predict_proba(X_test)[:,1]
        stack_test[:,j] = blend_test_tmp.mean(1)  # take average of the 8 fold results.
        logloss = log_loss(labels, stack_train[:,j])
        print "the log_loss should be:"
        print logloss
        cv_result.append((logloss, clf_name, j))

    # start stacking
    print "training first shot: stacking_1..."
    meta_clf = XGBC(num_round = 120, max_depth = 16, eta = 0.1, 
                                            min_child_weight = 2, nthread = 16)
    # all base features!
    new_train = np.column_stack((stack_train,train.values))
    new_test = np.column_stack((stack_test, test.values))
    meta_clf.fit(new_train, labels)
    stacking_1_pred = meta_clf.predict_proba(new_test)[:,1]

    stack_1_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_1_pred)})


    sorted_cv_results = sorted(cv_result, key= lambda x: x[0],reverse = False)
    best_single_model = sorted_cv_results[0]

    print "training first shot: stacking_2..."
    meta_clf = XGBC(num_round = 300, max_depth = 16, eta = 0.1, 
                                            min_child_weight = 2, nthread = 16)
    meta_clf.fit(new_train, labels)
    stacking_2_pred = meta_clf.predict_proba(new_test)[:,1]

    stack_2_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_2_pred)})


    # add some base feature sets here
    ## instruction sets
    print "training first shot: stacking_3..."
    base_instr_count = pd.read_csv("train_instr_frequency.csv")
    for n in list(base_instr_count)[1:]:
        if np.sum(base_instr_count[n]) == 0:
            del base_instr_count[n]
    base_instruction_fea = [x for x in list(base_instr_count) if x != 'Id']
    del base_instr_count
    new_train = np.column_stack((stack_train,train[base_instruction_fea + daf_fea].values))
    new_test = np.column_stack((stack_test, test[base_instruction_fea+daf_fea].values))

    meta_clf = XGBC(num_round = 120, max_depth = 16, eta = 0.1, 
                                            min_child_weight = 2, nthread = 16)
    meta_clf.fit(new_train, labels)
    stacking_3_pred = meta_clf.predict_proba(new_test)[:,1]

    stack_3_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_3_pred)})

    print "training first shot: stacking_4..."

    meta_clf = XGBC(num_round = 120, max_depth = 16, eta = 0.1, 
                                            min_child_weight = 2, nthread = 16)
    new_train = np.column_stack((stack_train,train[daf_fea].values))
    new_test = np.column_stack((stack_test, test[daf_fea].values))
    meta_clf.fit(new_train, labels)
    stacking_4_pred = meta_clf.predict_proba(new_test)[:,1]
    
    stack_4_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_4_pred)})    

    # best single model
    print "training third shot: the best single model..."
    print "%s has cross_validation logloss: %f" %(best_single_model[1],best_single_model[0])
    clf = model_list[best_single_model[2]][0]
    clf.fit(X, labels)
    single_pred = clf.predict_proba(X_test)[:,1]

    best_single_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(single_pred)})

    return best_single_result, stack_1_result, stack_2_result, stack_3_result, stack_4_result



def gen_submission(data_list, model_list):
    for train,test,c, daf_fea in data_list:
        print "starting doing class %i"%c
        if c == 1:
            best_single, stacking_1, stacking_2, stacking_3, stacking_4 = three_shots(model_list, train,test,c, daf_fea)
        else:
            single, s1, s2, s3, s4 = three_shots(model_list,train, test, c, daf_fea)
            best_single = pd.merge(best_single, single, on= 'Id', how='inner')
            stacking_1 = pd.merge(stacking_1, s1, on='Id',how='inner')
            stacking_2 = pd.merge(stacking_2, s2, on='Id',how='inner')
            stacking_3 = pd.merge(stacking_3, s3, on='Id',how='inner')
            stacking_4 = pd.merge(stacking_4, s4, on='Id',how='inner')


    best_single.to_csv('best_single.csv',index = False)
    stacking_1.to_csv('stacking_1.csv',index = False)
    stacking_2.to_csv('stacking_2.csv', index = False)
    stacking_3.to_csv('stacking_3.csv', index = False)
    stacking_4.to_csv('stacking_4.csv', index = False)


if __name__ == '__main__':
    model_list = get_model_list()
    data_list = gen_data()
    gen_submission(data_list, model_list)
    print "ALL DONE!!!"


