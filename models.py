from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.linear_model import LogisticRegression as LGR
from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.ensemble import ExtraTreesClassifier as ET
from xgboost_c import XGBC
from sklearn import cross_validation
from sklearn.cross_validation import StratifiedKFold as KFold
from sklearn.metrics import log_loss
import numpy as np
import pandas as pd

# create model_list
def get_model_list():
    model_list = [
        (RF(n_estimators=2000, n_jobs=-1, max_features = "log2",
                     random_state = 1234), 'rf2000log2'),
        (RF(n_estimators=2000, n_jobs=-1, max_features = "sqrt",
                     random_state = 1234), 'rf2000sqrt'),
        (ET(n_estimators=2000, n_jobs=-1, max_features = "log2",
                     random_state = 1234), 'et2000log2'),
        (ET(n_estimators=2000, n_jobs=-1, max_features = "sqrt",
                     random_state = 1234), 'et2000sqrt'),
        (LGR(random_state=1234),'lgr')
    ]

    for num_round in [150, 300]:
        for max_depth in [2, 5, 8, 12, 15, 20]:
            for eta in [0.1,0.25,0.5]:
                for min_child_weight in [2,5,8]:
                    model_list.append((XGBC(num_round = num_round, max_depth = max_depth, eta = eta, 
                                            min_child_weight = min_child_weight, nthread = 8),
                                       'xgb_tree_%i_depth_%i_lr_%i_child_%i'%(num_round, max_depth, eta, min_child_weight)))
    return model_list


def gen_data():
    # ngram datasets
    train_data = pd.read_csv("grams_train.csv")
    test_data = pd.read_csv("grams_test.csv")

    train_label = pd.read_csv("trainLabels.csv")
    
    # single byte frequency datasets
    train_frequency = pd.read_csv("train_frequency.csv")
    test_frequency = pd.read_csv("test_frequency.csv")
    train_frequency.ix[:,1:] = train_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_frequency = train_frequency.replace(np.inf, 0)
    test_frequency.ix[:,1:]=test_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_frequency = test_frequency.replace(np.inf, 0)

    # daf features
    train_daf = pd.read_csv("train_daf.csv")
    test_daf = pd.read_csv("test_daf.csv")
    daf_list = [0,165,91,60,108,84,42,93,152,100]

    # instr freq features
    train_instr_freq = pd.read_csv("train_instr_frequency.csv")
    test_instr_freq = pd.read_csv("test_instr_frequency.csv")
    for n in list(train_instr_freq)[1:]:
        if np.sum(train_instr_freq[n]) == 0:
            del train_instr_freq[n]
            del test_instr_freq[n]

    train_instr_freq.ix[:,1:] = train_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_instr_freq = train_instr_freq.replace(np.inf, 0)
    test_instr_freq.ix[:,1:]=test_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_instr_freq = test_instr_freq.replace(np.inf, 0)


    # dll features
    train_dll = pd.read_csv("train_dll.csv")
    test_dll = pd.read_csv("test_dll.csv")


    class_copy = train_label.copy()
    for c in range(1,10):
        class_copy['Class'] = [int(i == c) for i in train_label['Class']]
        train = pd.merge(train_data.ix[:,[0] + range((c-1)*500+1, c*500+1)], class_copy, on='Id',how='inner')
        test = test_data.ix[:,[0] + range((c-1)*500+1, c*500+1)]
        
        # merge byte frequency

        train = pd.merge(train, train_frequency, on='Id')
        test = pd.merge(test, test_frequency, on='Id')
        
        # merge daf
        train = pd.merge(train, train_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')
        test = pd.merge(test, test_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')

        # merge instr freq
        train = pd.merge(train, train_instr_freq, on='Id')
        test = pd.merge(test, test_instr_freq, on='Id')

        # merge dll
        train = pd.merge(train, train_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')
        test = pd.merge(test, test_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')

        yield train, test, c


'''
now the three_shots is actually 5 shots! one with best single model, the others are correpsonding to 
different penalty strength logistic regression.
'''
def three_shots(model_list, train, test, c):
    '''
    three_shots includes one cross_validation stacking, one stacking with better half, 
    and one single shot with best cross_validation results.
    '''
    # read in data
    print "read data and prepare modelling..."
    X = train
    Id = X.Id
    labels = X.Class
    del X['Id']
    del X['Class']
    X = X.as_matrix()
    X_test = test
    Id_test = X_test.Id
    del X_test['Id']
    X_test = X_test.as_matrix()
    kf = KFold(labels, n_folds=4)  # 4 folds

    # create cv meta data
    stack_train = np.zeros((len(Id),len(model_list)))
    stack_test = np.zeros((len(Id_test),len(model_list)))

    cv_result = list()  # store the cross validation scores.
    for j, (clf, clf_name) in enumerate(model_list):
        print 'Training classifier %s' %clf_name
        blend_test_tmp = np.zeros((X_test.shape[0],len(kf)))
        for i, (train, validate) in enumerate(kf):
            X_train, X_validate, labels_train, labels_validate = X[train,:], X[validate,:], labels[train], labels[validate]
            clf.fit(X_train,labels_train)
            stack_train[validate,j] = clf.predict_proba(X_validate)[:,1]
            blend_test_tmp[:,i] = clf.predict_proba(X_test)[:,1]
        stack_test[:,j] = blend_test_tmp.mean(1)  # take average of the 8 fold results.
        logloss = log_loss(labels, stack_train[:,j])
        print "the log_loss should be:"
        print logloss
        cv_result.append((logloss, clf_name, j))

    # start stacking
    print "training first shot: stacking_1..."
    meta_clf = LGR(C=0.75, random_state=1234)
    meta_clf.fit(stack_train, labels)
    stacking_1_pred = meta_clf.predict_proba(stack_test)[:,1]

    stack_1_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_1_pred)})


    sorted_cv_results = sorted(cv_result, key= lambda x: x[0],reverse = False)
    best_single_model = sorted_cv_results[0]

    print "training first shot: stacking_2..."
    meta_clf = LGR(C=0.5, random_state=1234)
    meta_clf.fit(stack_train, labels)
    stacking_2_pred = meta_clf.predict_proba(stack_test)[:,1]

    stack_2_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_2_pred)})

    print "training first shot: stacking_3..."
    meta_clf = LGR(C=0.25, random_state=1234)
    meta_clf.fit(stack_train, labels)
    stacking_3_pred = meta_clf.predict_proba(stack_test)[:,1]

    stack_3_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_3_pred)})

    print "training first shot: stacking_3..."
    meta_clf = LGR(C=0.1, random_state=1234)
    meta_clf.fit(stack_train, labels)
    stacking_4_pred = meta_clf.predict_proba(stack_test)[:,1]
    
    stack_4_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_4_pred)})    

    # best single model
    print "training third shot: the best single model..."
    print "%s has cross_validation logloss: %f" %(best_single_model[1],best_single_model[0])
    clf = model_list[best_single_model[2]][0]
    clf.fit(X, labels)
    single_pred = clf.predict_proba(X_test)[:,1]

    best_single_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(single_pred)})

    return best_single_result, stack_1_result, stack_2_result, stack_3_result, stack_4_result



def gen_submission(data_list, model_list):
    for train,test,c in data_list:
        print "starting doing class %i"%c
        if c == 1:
            best_single, stacking_1, stacking_2, stacking_3, stacking_4 = three_shots(model_list, train,test,c)
        else:
            single, s1, s2, s3, s4 = three_shots(model_list,train, test, c)
            best_single = pd.merge(best_single, single, on= 'Id', how='inner')
            stacking_1 = pd.merge(stacking_1, s1, on='Id',how='inner')
            stacking_2 = pd.merge(stacking_2, s2, on='Id',how='inner')
            stacking_3 = pd.merge(stacking_3, s3, on='Id',how='inner')
            stacking_4 = pd.merge(stacking_4, s4, on='Id',how='inner')


    best_single.to_csv('best_single.csv',index = False)
    stacking_1.to_csv('stacking_1.csv',index = False)
    stacking_2.to_csv('stacking_2.csv', index = False)
    stacking_3.to_csv('stacking_3.csv', index = False)
    stacking_4.to_csv('stacking_4.csv', index = False)


if __name__ == '__main__':
    model_list = get_model_list()
    data_list = gen_data()
    gen_submission(data_list, model_list)
    print "ALL DONE!!!"


