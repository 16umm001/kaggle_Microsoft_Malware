from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.linear_model import LogisticRegression as LGR
from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.ensemble import ExtraTreesClassifier as ET
from xgboost_c import XGBC
from sklearn import cross_validation
from sklearn.cross_validation import StratifiedKFold as KFold
from sklearn.metrics import log_loss
import numpy as np
import pandas as pd

model_list = [
    (RF(n_estimators=1500, n_jobs=-1, max_features = "log2",
                 random_state = 1234), 'rf1500log2'),
    (RF(n_estimators=1500, n_jobs=-1, max_features = "sqrt",
                 random_state = 1234), 'rf500sqrt'),
    (ET(n_estimators=1500, n_jobs=-1, max_features = "log2",
                 random_state = 1234), 'et1500log2'),
    (ET(n_estimators=1500, n_jobs=-1, max_features = "sqrt",
                 random_state = 1234), 'et1500sqrt'),
    (XGBC(num_round = 150, max_depth = 4, eta= 0.25, nthread = 8),'xgb4.25'),
    (XGBC(num_round = 150, max_depth = 10, eta= 0.25, nthread = 8),'xgb10.25'),
    (XGBC(num_round = 150, max_depth = 20, eta= 0.25, nthread = 8),'xgb20.25'),
    (XGBC(num_round = 150, max_depth = 25, eta= 0.25, nthread = 8),'xgb25.25'),
    (XGBC(num_round = 150, max_depth = 10, eta= 0.1, nthread = 8),'xgb10.1'),
    (XGBC(num_round = 150, max_depth = 10, eta= 0.5, nthread = 8),'xgb10.5'),
    (XGBC(num_round = 150, max_depth = 20, eta= 0.25, min_child_weight = 4, nthread = 8),'xgb20.25_4'),
    (XGBC(num_round = 150, max_depth = 30, eta= 0.15, min_child_weight = 6, nthread = 8),'xgb30.15_6'),
    (XGBC(num_round = 150, max_depth = 10, eta= 0.1, min_child_weight = 8, nthread = 8),'xgb10.1_8'),
    (XGBC(num_round = 300, max_depth = 8, eta= 0.1, min_child_weight = 4, nthread = 8),'xgb300_20.25_4'),
    (XGBC(num_round = 300, max_depth = 15, eta= 0.1, min_child_weight = 2, nthread = 8),'xgb300_30.25_2'),
    (XGBC(num_round = 300, max_depth = 10, eta= 0.1, min_child_weight = 2, nthread = 8),'xgb300_10.1_2'),
    (LGR(random_state=1234),'lgr')
]


def gen_data():
    # ngram datasets
    train_data = pd.read_csv("grams_train.csv")
    test_data = pd.read_csv("grams_test.csv")

    train_label = pd.read_csv("trainLabels.csv")
    
    # single byte frequency datasets
    train_frequency = pd.read_csv("train_frequency.csv")
    test_frequency = pd.read_csv("test_frequency.csv")
    train_frequency.ix[:,1:] = train_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_frequency = train_frequency.replace(np.inf, 0)
    test_frequency.ix[:,1:]=test_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_frequency = test_frequency.replace(np.inf, 0)

    # daf features
    train_daf = pd.read_csv("train_daf.csv")
    test_daf = pd.read_csv("test_daf.csv")
    daf_list = [0,165,91,60,108,84,42,93,152,100]

    # instr freq features
    train_instr_freq = pd.read_csv("train_instr_frequency.csv")
    test_instr_freq = pd.read_csv("test_instr_frequency.csv")
    for n in list(train_instr_freq)[1:]:
        if np.sum(train_instr_freq[n]) == 0:
            del train_instr_freq[n]
            del test_instr_freq[n]

    train_instr_freq.ix[:,1:] = train_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_instr_freq = train_instr_freq.replace(np.inf, 0)
    test_instr_freq.ix[:,1:]=test_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_instr_freq = test_instr_freq.replace(np.inf, 0)


    # dll features
    train_dll = pd.read_csv("train_dll.csv")
    test_dll = pd.read_csv("test_dll.csv")


    class_copy = train_label.copy()
    for c in range(1,10):
        class_copy['Class'] = [int(i == c) for i in train_label['Class']]
        train = pd.merge(train_data.ix[:,[0] + range((c-1)*500+1, c*500+1)], class_copy, on='Id',how='inner')
        test = test_data.ix[:,[0] + range((c-1)*500+1, c*500+1)]
        
        # merge byte frequency

        train = pd.merge(train, train_frequency, on='Id')
        test = pd.merge(test, test_frequency, on='Id')
        
        # merge daf
        train = pd.merge(train, train_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')
        test = pd.merge(test, test_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')

        # merge instr freq
        train = pd.merge(train, train_instr_freq, on='Id')
        test = pd.merge(test, test_instr_freq, on='Id')

        # merge dll
        train = pd.merge(train, train_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')
        test = pd.merge(test, test_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')

        yield train, test, c

def three_shots(model_list, train, test, c):
    '''
    three_shots includes one cross_validation stacking, one stacking with better half, 
    and one single shot with best cross_validation results.
    '''
    # read in data
    print "read data and prepare modelling..."
    X = train
    Id = X.Id
    labels = X.Class
    del X['Id']
    del X['Class']
    X = X.as_matrix()
    X_test = test
    Id_test = X_test.Id
    del X_test['Id']
    X_test = X_test.as_matrix()
    kf = KFold(labels, n_folds=4)  # 4 folds

    # create cv meta data
    stack_train = np.zeros((len(Id),len(model_list)))
    stack_test = np.zeros((len(Id_test),len(model_list)))

    cv_result = list()  # store the cross validation scores.
    for j, (clf, clf_name) in enumerate(model_list):
        print 'Training classifier %s' %clf_name
        blend_test_tmp = np.zeros((X_test.shape[0],len(kf)))
        for i, (train, validate) in enumerate(kf):
            X_train, X_validate, labels_train, labels_validate = X[train,:], X[validate,:], labels[train], labels[validate]
            clf.fit(X_train,labels_train)
            stack_train[validate,j] = clf.predict_proba(X_validate)[:,1]
            blend_test_tmp[:,i] = clf.predict_proba(X_test)[:,1]
        stack_test[:,j] = blend_test_tmp.mean(1)  # take average of the 8 fold results.
        logloss = log_loss(labels, stack_train[:,j])
        print "the log_loss should be:"
        print logloss
        cv_result.append((logloss, clf_name, j))

    # start stacking
    print "training first shot: stacking..."
    meta_clf = LGR(C=0.75, random_state=1234)
    meta_clf.fit(stack_train, labels)
    stacking_pred = meta_clf.predict_proba(stack_test)[:,1]

    stack_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(stacking_pred)})


    sorted_cv_results = sorted(cv_result, key= lambda x: x[0],reverse = False)
    best_single_model = sorted_cv_results[0]
    '''
    better_half_index = [x[2] for x in sorted_cv_results[:len(model_list)/2]]
    half_stack_train = stack_train[:,better_half_index]
    half_stack_test = stack_test[:,better_half_index]

    # start stacking better half
    print "training second shot: half stacking..."
    meta_clf.fit(half_stack_train,labels)
    half_stack_pred = meta_clf.predict_proba(half_stack_test)[:,1]

    half_stack_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(half_stack_pred)})
    '''
    print "training first shot: half stacking..."
    meta_clf = LGR(C=0.5, random_state=1234)
    meta_clf.fit(stack_train, labels)
    half_stack_pred = meta_clf.predict_proba(stack_test)[:,1]

    half_stack_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(half_stack_pred)})    
    # best single model
    print "training third shot: the best single model..."
    print "%s has cross_validation logloss: %f" %(best_single_model[1],best_single_model[0])
    clf = model_list[best_single_model[2]][0]
    clf.fit(X, labels)
    single_pred = clf.predict_proba(X_test)[:,1]

    best_single_result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(single_pred)})

    return best_single_result, half_stack_result, stack_result


def gen_submission(data_list, model_list):
    for train,test,c in data_list:
        print "starting doing class %i"%c
        if c == 1:
            best_single, half_stacking, stacking = three_shots(model_list, train,test,c)
        else:
            single, half, stack = three_shots(model_list,train, test, c)
            best_single = pd.merge(best_single, single, on= 'Id', how='inner')
            half_stacking = pd.merge(half_stacking, half, on='Id',how='inner')
            stacking = pd.merge(stacking, stack, on='Id',how='inner')
    best_single.to_csv('best_single.csv',index = False)
    half_stacking.to_csv('half_stacking.csv',index = False)
    stacking.to_csv('stacking.csv', index = False)

if __name__ == '__main__':
    data_list = gen_data()
    gen_submission(data_list, model_list)
    print "ALL DONE!!!"


