from os.path import isfile, join
import os
from multiprocessing import Pool
from csv import writer
import pandas as pd

paths = ['train','test']

def get_size(path):
	Files = os.listdir(path)
	byteFiles = [i for i in Files if '.bytes' in i]
	asmFiles = [i for i in Files if '.asm' in i]
	result = path + '_file_size.csv'

	with open(result, 'wb') as f:
		fw = writer(f)
		colnames = ['Id', 'byte_size','asm_size']
		fw.writerow(colnames)

		for t, fname in enumerate(byteFiles):
			byte_size = [os.path.getsize(path+'/'+fname)]
			asm_size = [os.path.getsize(path+'/'+fname[:fname.find('.bytes')] + '.asm')]
			row =[fname[:fname.find('.bytes')]] + byte_size + asm_size

			fw.writerow(row)

			if (t+1)%100==0:
				print(t+1, 'files loaded for ', path)

def norm_zero_one():
	train = pd.read_csv("train_file_size.csv")
	test = pd.read_csv("test_file_size.csv")
	max_byte = float(max(train['byte_size']))
	min_byte = float(min(train['byte_size']))
	max_asm = float(max(train['asm_size']))
	min_asm = float(min(train['asm_size']))
	train['byte_size'] = (train['byte_size'] - min_byte)/(max_byte - min_byte)
	train['asm_size'] = (train['asm_size'] - min_asm)/(max_asm - min_asm)
	test['byte_size'] = (test['byte_size'] - min_byte)/(max_byte - min_byte)
	test['asm_size'] = (test['asm_size'] - min_asm)/(max_asm - min_asm)	
	train.to_csv('train_file_size.csv', index = False)
	test.to_csv('test_file_size.csv', index = False)

if __name__ == '__main__':
	p = Pool(2)
	p.map(get_size, paths)
	norm_zero_one()