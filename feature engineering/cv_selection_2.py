from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.linear_model import LogisticRegression as LGR
from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.ensemble import ExtraTreesClassifier as ET
from xgboost_c import XGBC
from sklearn import cross_validation
from sklearn.cross_validation import StratifiedKFold as KFold
from sklearn.metrics import log_loss
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfTransformer
import pickle
from sklearn.linear_model import SGDClassifier as SGD


'''
Like cv_selection.py, just add one more function, spe_fea, which will generate specific features 
for corresponding class.

HOWEVER, It doesn't work well at all.
'''

# create model_list

def get_model_list():
    model_list = []
    for num_round in [150]:
        for max_depth in [8]:
            for eta in [0.5]:
                for min_child_weight in [2]:
                    model_list.append((XGBC(num_round = num_round, max_depth = max_depth, eta = eta, 
                                            min_child_weight = min_child_weight, nthread = 16),
                                       'xgb_tree_%i_depth_%i_lr_%f_child_%i'%(num_round, max_depth, eta, min_child_weight)))
    return model_list


# this one generate common features among 9 classes.
def gen_data():
    # the 4k features!
    the_train = pickle.load(open('X33_train.p','rb'))  
    the_test = pickle.load(open('X33_test.p','rb'))
    # corresponding id
    Id = pickle.load(open('xid.p','rb'))
    #labels = pickle.load(open('y.p','rb'))    
    Id_test = pickle.load(open('Xt_id.p','rb'))

    # merge them into pandas
    join_train = np.column_stack((Id, the_train))
    join_test = np.column_stack((Id_test, the_test))
    train = pd.DataFrame(join_train, columns=['Id']+['the_fea%i'%i for i in xrange(the_train.shape[1])])
    test = pd.DataFrame(join_test, columns=['Id']+['the_fea%i'%i for i in xrange(the_train.shape[1])])
    del join_train, join_test
    # convert into numeric features
    train = train.convert_objects(convert_numeric=True)
    test = test.convert_objects(convert_numeric=True)


    # including more things
    train_count = pd.read_csv("train_frequency.csv")
    test_count = pd.read_csv("test_frequency.csv")

    train = pd.merge(train, train_count, on='Id')
    test = pd.merge(test, test_count, on='Id')

    train_file_size = pd.read_csv("train_file_size.csv")
    test_file_size = pd.read_csv("test_file_size.csv")
    train = pd.merge(train, train_file_size, on='Id')
    test = pd.merge(test, test_file_size, on='Id')     
    
    return train, test

# this one generate class specific features, and labels
def spe_fea():
    '''
    train_daf = pd.read_csv("train_daf.csv")
    test_daf = pd.read_csv("test_daf.csv")
    daf_list = [0,165,91,60,108,84,42,93,152,100] #daf list for 500 grams.
    '''
    train_instr_count = pd.read_csv("train_instr_frequency.csv")
    test_instr_count = pd.read_csv("test_instr_frequency.csv")
    for n in list(train_instr_count)[1:]:
        if np.sum(train_instr_count[n]) == 0:
            del train_instr_count[n]
            del test_instr_count[n]

    train_label = pd.read_csv("trainLabels.csv")
    class_copy = train_label.copy()
    for c in range(1,10):
        class_copy['Class'] = [int(i == c) for i in train_label['Class']]
        #semi_copy['Class'] = [int(i == c) for i in semi_label['Class']]
        train = pd.merge(train_instr_count, class_copy, on='Id',how='inner')
        test = test_instr_count

        yield train, test, c



def cross_validate(model_list):

    # read in data
    print "read data and prepare modelling..."

    class_divid = pd.read_csv("trainLabels.csv")
    kf = KFold(class_divid.Class.values, n_folds=4)  # 4 folds

    train_common, test_common = gen_data()

    # for every model, fit 9 times and get a cv result.
    for j, (clf, clf_name) in enumerate(model_list):
        stack_train = np.zeros((train_common.shape[0],9)) # 9 classes.   
        labels_true = np.zeros((train_common.shape[0],9))
        spec_list = spe_fea() 
        for train_spec, test_spec, c in spec_list:
            print "modelling %s for class %i"%(clf_name,c)
            train = pd.merge(train_common, train_spec, on='Id', how = 'inner')
            test = pd.merge(test_common, test_spec, on='Id', how = 'inner')
            X = train
            Id = X.Id
            labels = X.Class # for the purpose of using multilogloss fun.
            del X['Id']
            del X['Class']
            X = X.as_matrix()
            X_test = test
            id_test = X_test.Id
            del X_test['Id']
            X_test = X_test.as_matrix()

            for i, (train_fold, validate) in enumerate(kf):
                X_train, X_validate, labels_train, labels_validate = X[train_fold,:], X[validate,:], labels[train_fold], labels[validate]            
                clf.fit(X_train,labels_train)
                stack_train[validate,c-1] = clf.predict_proba(X_validate)[:,1]
            print "the single class logloss: %f"%log_loss(labels, stack_train[:,c-1])
            labels_true[:,c-1] = labels
        print multiclass_log_loss(labels_true, stack_train)

def multiclass_log_loss(y_true, y_pred, eps=1e-15):
    """Multi class version of Logarithmic Loss metric.
    https://www.kaggle.com/wiki/MultiClassLogLoss

    Parameters
    ----------
    y_true : array, shape = [n_samples]
            true class, intergers in [0, n_classes - 1)
    y_pred : array, shape = [n_samples, n_classes]

    Returns
    -------
    loss : float
    """
    predictions = np.clip(y_pred, eps, 1 - eps)

    # normalize row sums to 1
    predictions /= predictions.sum(axis=1)[:, np.newaxis]

    actual = y_true.astype(int)
    #actual = np.zeros(y_pred.shape)
    n_samples = actual.shape[0]
    #actual[np.arange(n_samples), y_true.astype(int)] = 1
    vectsum = np.sum(actual * np.log(predictions))
    loss = -1.0 / n_samples * vectsum
    return loss


if __name__ == '__main__':
    model_list = get_model_list()
    cross_validate(model_list)
    print "ALL DONE!!!"


