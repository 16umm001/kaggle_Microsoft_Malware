from sklearn.ensemble import RandomForestClassifier as RF
#from sklearn.linear_model import LogisticRegression as LGR
from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.ensemble import ExtraTreesClassifier as ET
from xgboost_c import XGBC
from sklearn import cross_validation
from sklearn.cross_validation import StratifiedKFold as KFold
from sklearn.metrics import log_loss
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfTransformer


# create model_list

def get_model_list():
    model_list = []

    for num_round in [150]:
        for max_depth in [2]:
            for eta in [0.25]:
                for min_child_weight in [2]:
                    model_list.append((XGBC(num_round = num_round, max_depth = max_depth, eta = eta, 
                                            min_child_weight = min_child_weight, nthread = 16),
                                       'xgb_tree_%i_depth_%i_lr_%f_child_%i'%(num_round, max_depth, eta, min_child_weight)))
    return model_list



def gen_data(c):
    # ngram datasets
    train_data = pd.read_csv("train_data_750.csv")
    test_data = pd.read_csv("test_data_750.csv")

    train_label = pd.read_csv("trainLabels.csv")
    semi_label = pd.read_csv('semi_labels.csv')
    
    ### three types: count, count_tf, frequency

    # single byte frequency datasets
    train_count = pd.read_csv("train_frequency.csv")
    test_count = pd.read_csv("test_frequency.csv")
    '''
    train_count_tf = train_count.copy()
    test_count_tf = test_count.copy()

    # tf idf 
    tf_idf = TfidfTransformer()
    train_count_tf.ix[:,1:] = tf_idf.fit_transform(train_count_tf.ix[:,1:]).toarray()
    test_count_tf.ix[:,1:] = tf_idf.transform(test_count_tf.ix[:,1:]).toarray()
    '''
    train_frequency = train_count.copy()
    test_frequency = test_count.copy()
    train_frequency.ix[:,1:] = train_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_frequency = train_frequency.replace(np.inf, 0)
    test_frequency.ix[:,1:]=test_frequency.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_frequency = test_frequency.replace(np.inf, 0)


    # daf features
    train_daf = pd.read_csv("train_daf.csv")
    test_daf = pd.read_csv("test_daf.csv")
    daf_list = [0,165,91,60,108,84,42,93,152,100] #daf list for 500 grams.
    #daf_list = [0, 122, 39, 64, 83, 63, 34, 83, 85,86]  # daf list for 1000 grams.


    # three types: count, count_tf, frequency
    # instr freq features
    train_instr_count = pd.read_csv("train_instr_frequency.csv")
    test_instr_count = pd.read_csv("test_instr_frequency.csv")
    for n in list(train_instr_count)[1:]:
        if np.sum(train_instr_count[n]) == 0:
            del train_instr_count[n]
            del test_instr_count[n]
    '''
    train_instr_count_tf = train_instr_count.copy()
    test_instr_count_tf  = test_instr_count.copy()

    tf_idf = TfidfTransformer()
    train_instr_count_tf.ix[:,1:] = tf_idf.fit_transform(train_instr_count_tf.ix[:,1:]).toarray()
    test_instr_count_tf.ix[:,1:] = tf_idf.transform(test_instr_count_tf.ix[:,1:]).toarray()
    '''

    train_instr_freq = train_instr_count.copy()
    test_instr_freq = test_instr_count.copy()

    train_instr_freq.ix[:,1:] = train_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    train_instr_freq = train_instr_freq.replace(np.inf, 0)
    test_instr_freq.ix[:,1:]=test_instr_freq.ix[:,1:].apply(lambda x: x/np.sum(x), axis = 1)
    test_instr_freq = test_instr_freq.replace(np.inf, 0)


    # dll features
    train_dll = pd.read_csv("train_dll.csv")
    test_dll = pd.read_csv("test_dll.csv")

    # file size 
    train_file_size = pd.read_csv("train_file_size.csv")
    test_file_size = pd.read_csv("test_file_size.csv")


    ## these are results based on single feature set cv results.
    '''
    byte_selection = {1: 'count', 2: 'tf', 3: 'count', 4: 'count',
                    5: 'count',6: 'freq', 7:'count', 8:'freq', 9:'tf'}

    instr_selection = {1: 'count', 2: 'tf', 3: 'tf', 4: 'tf',
                    5: 'count',6: 'freq', 7:'count', 8:'freq', 9:'tf'}
    '''

    class_copy = train_label.copy()
    semi_copy = semi_label.copy()
    class_copy['Class'] = [int(i == c) for i in train_label['Class']]
    semi_copy['Class'] = [int(i == c) for i in semi_label['Class']]
    train = pd.merge(train_data.ix[:,[0] + range((c-1)*750+1, c*750+1)], class_copy, on='Id',how='inner')
    test = test_data.ix[:,[0] + range((c-1)*750+1, c*750+1)]
    
    # merge byte frequency
    '''
    if byte_selection[c] == 'count':
        train_byte_features = train_count
        test_byte_features = test_count
    elif byte_selection[c] == 'tf':
        train_byte_features = train_count_tf
        test_byte_features = test_count_tf
    elif byte_selection[c] == 'freq':
        train_byte_features = train_frequency
        test_byte_features = test_frequency
    '''
    train_byte_features = train_frequency
    test_byte_features = test_frequency

    train = pd.merge(train, train_byte_features, on='Id')
    test = pd.merge(test, test_byte_features, on='Id')
    
    # merge daf
    train = pd.merge(train, train_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')
    test = pd.merge(test, test_daf.ix[:, [0] +range(sum(daf_list[:c])+1, sum(daf_list[:c+1])+1)], on='Id')

    '''
    # merge instr freq
    if instr_selection[c] == 'count':
        train_instr_features = train_instr_count
        test_instr_features = test_instr_count
    elif instr_selection[c] == 'tf':
        train_instr_features = train_instr_count_tf
        test_instr_features = test_instr_count_tf
    elif instr_selection[c] == 'freq':
        train_instr_features = train_instr_freq
        test_instr_features = test_instr_freq   
    '''
    train_instr_features = train_instr_freq
    test_instr_features = test_instr_freq   

    train = pd.merge(train, train_instr_features, on='Id')
    test = pd.merge(test, test_instr_features, on='Id')

    # merge dll
    train = pd.merge(train, train_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')
    test = pd.merge(test, test_dll.ix[:, [0] + range((c-1)*500+1, c*500+1)], on='Id')

    # merge file size
    train = pd.merge(train, train_file_size, on='Id')
    test = pd.merge(test, test_file_size, on='Id')    
    
    ### concat train and semi test datasets!
    semi_test = pd.merge(test, semi_copy, on='Id', how='inner')
    train = pd.concat([train, semi_test], ignore_index=True)    

    return train, test



def single_cv(data_list, model_list, class_label):
    '''
    three_shots includes one cross_validation stacking, one stacking with better half, 
    and one single shot with best cross_validation results.
    '''
    # read in data
    train, test = data_list
    c = class_label
    #print "starting doing class %i"%c
    #print "read data and prepare modelling..."
    X = train
    Id = X.Id
    labels = X.Class
    del X['Id']
    del X['Class']
    X = X.as_matrix()
    X_test = test
    Id_test = X_test.Id
    del X_test['Id']
    X_test = X_test.as_matrix()
    kf = KFold(labels, n_folds=4)  # 4 folds

    # create cv meta data
    stack_train = np.zeros((len(Id),len(model_list)))
    #stack_test = np.zeros((len(Id_test),len(model_list)))

    cv_result = list()  # store the cross validation scores.
    for j, (clf, clf_name) in enumerate(model_list):
        #print 'Training classifier %s' %clf_name
        #blend_test_tmp = np.zeros((X_test.shape[0],len(kf)))
        for i, (train, validate) in enumerate(kf):
            X_train, X_validate, labels_train, labels_validate = X[train,:], X[validate,:], labels[train], labels[validate]
            clf.fit(X_train,labels_train)
            stack_train[validate,j] = clf.predict_proba(X_validate)[:,1]
        #    blend_test_tmp[:,i] = clf.predict_proba(X_test)[:,1]
        #stack_test[:,j] = blend_test_tmp.mean(1)  # take average of the 8 fold results.
        logloss = log_loss(labels, stack_train[:,j])
        #print "the log_loss should be:"
        #print logloss
        cv_result.append((logloss, clf_name, j))

    sorted_cv_results = sorted(cv_result, key= lambda x: x[0],reverse = False)
    best_single_model = sorted_cv_results[0]
    print "%s has cross_validation logloss: %f" %(best_single_model[1],best_single_model[0])
    #print stack_train.shape, stack_test.shape

    return stack_train, labels
    #return stack_train, stack_test, labels, Id_test




def multi_cv(train,y_true,model_list):
    best_logloss = 1.
    best_models = list()
    mdl = [m[1] for m in model_list]
    for i1 in xrange(len(mdl)):
        for i2 in xrange(len(mdl)):
            for i3 in xrange(len(mdl)):
                for i4 in xrange(len(mdl)):
                    for i5 in xrange(len(mdl)):
                        for i6 in xrange(len(mdl)):
                            for i7 in xrange(len(mdl)):
                                for i8 in xrange(len(mdl)):
                                    for i9 in xrange(len(mdl)):
                                        current_model = [mdl[i1], mdl[i2], mdl[i3], mdl[i4], mdl[i5], mdl[i6], mdl[i7], mdl[i8],mdl[i9]]
                                        tmp_array = np.column_stack((train[0][:,i1],train[1][:,i2],train[2][:,i3],train[3][:,i4],train[4][:,i5],train[5][:,i6],train[6][:,i7],train[7][:,i8],train[8][:,i9]))
                                        current_score = multiclass_log_loss(tmp_array, y_true)
                                        if current_score < best_logloss:
                                            best_logloss = current_score
                                            best_models = current_model
                                            print "improve the score!! And the model set is..."
                                            print best_logloss, best_models
    return best_models


def generate_pred(data_list, model_list,c, model_index):
    # read in data
    train, test = data_list
    #print "starting doing class %i"%c
    #print "read data and prepare modelling..."
    X = train
    Id = X.Id
    labels = X.Class
    del X['Id']
    del X['Class']
    X = X.as_matrix()
    X_test = test
    Id_test = X_test.Id
    del X_test['Id']
    X_test = X_test.as_matrix()
    clf = model_list[model_index][0]
    clf.fit(X, labels)
    pred = clf.predict_proba(X_test)[:,1]
    result = pd.DataFrame(data={'Id':list(Id_test), 'Prediction%i'%c:list(pred)})
    return result

def multiclass_log_loss(y_pred, y_true, eps=1e-15):
    """Multi class version of Logarithmic Loss metric.
    https://www.kaggle.com/wiki/MultiClassLogLoss

    Parameters
    ----------
    # y_true is just given here
    y_true : array, shape = [n_samples]
            true class, intergers in [0, n_classes - 1]

    y_pred : array, shape = [n_samples, n_classes]

    Returns
    -------
    loss : float
    """
    #y_true = pd.read_csv("trainLabels.csv").Class.values - 1
    predictions = np.clip(y_pred, eps, 1 - eps)

    # normalize row sums to 1
    predictions /= predictions.sum(axis=1)[:, np.newaxis]

    actual = y_true.astype(int)
    #actual = np.zeros(y_pred.shape)
    n_samples = actual.shape[0]
    #actual[np.arange(n_samples), y_true.astype(int)] = 1
    vectsum = np.sum(actual * np.log(predictions))
    loss = -1.0 / n_samples * vectsum
    return loss


if __name__ == '__main__':
    model_list = get_model_list()
    train_all = list()
    test_all = list()
    for c in xrange(1,10):
        print "doing %i ..."%c
        data_list = gen_data(c)
        #stack_train, stack_test, labels = single_cv(data_list, model_list,1)
        stack_train, labels = single_cv(data_list, model_list,1)
        if c == 1:
            labels_all = labels
        else:
            labels_all = np.column_stack((labels_all, labels))
        train_all.append(stack_train)
        #test_all.append(stack_test)
    
    best_selected_models = multi_cv(train = train_all,y_true = labels_all,model_list = model_list)

    print "obtain testing sets..."
    model_names = [n[1] for n in model_list]
    for i in xrange(0,9):
        data_list = gen_data(i+1)
        tmp_index = model_names.index(best_selected_models[i])
        print "starting doing %s for class %i"%(model_list[tmp_index][1], i+1)
        if i == 0:
            result = generate_pred(data_list, model_list, i+1, tmp_index)
        else:
            tmp_result = generate_pred(data_list, model_list, i+1, tmp_index)
            result = pd.merge(result, tmp_result)

    result.to_csv("multi_best_models_750_selected.csv", index = False)



    print "ALL DONE!!!"



