# kaggle_Microsoft_Malware
code for kaggle competition Microsoft malware classification

## features
mainly the feature extracted based on [this paper](http://www.utdallas.edu/~bxt043000/Publications/Journal-Papers/DAS/J51_A_Scalable_Multilevel_Feature_Extraction_Technique_to_Detect_Malicious_Executables.pdf)

`unique_gram.py` and `join_grams.py` are used to extract the 4-grams of byte files based on info gain.
top 500, 750, 1000 are tried, and 750 works best for LB, 1000 seems better based on cross validation.

`daf.py` extracts the DAF features from asm files according to the paper. 

`dll.py` extracts the functions from asm file (which I am not sure if it is the DLL feature according to the paper)

Also, file size, instruction count frequency, single byte count frequency are included as well.

## models
It is two level modelling, within one script `models.py`.

the function `get_model_list` takes all models for level 1 learning. I wrote a simple script `xgboost_c.py`
to make xgboost's calling function same as others in sklearn.

`gen_data` takes all the feature sets described above. 

`three_shots` does the level 2 learning. It was intending to output 3 submission files, but xgboost always outperformed others so I tend to just use xgboost for now.

Level 2 is just stacking the level 1 result of 4 fold cross validate. Instead of classifer 9 classes at the same time, it does prediction for each class, respectively. Instead of just stacking the level 1 predictions, including the original features does boost the performance! So level 1 predictions can be just viewed as part of feature engineering.

`multi_models.py` is just used to see the multi log loss.

## Calibrate
Since all classes are fitted independently, calibration should give a better performance. My calibration is simple.
I observed that class 1 has the smallest logloss, 0.0007, while the others have 0.00X and Class 8 has 0.01X. So there is two stpes to calibrate the result.

1. calibrate the class 8 based on other predictions, that is, pred of class 8 = alpha * class 8 + (1 - alpha)*(1 - sum of others)

2. Fully trust class 1, make all others sum up to 1 - pred of class 1.

Code can be found in `cali_1.py`

## some more ideas.
To be coming..